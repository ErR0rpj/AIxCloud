# Introdution to Generative AI

## Supervised Learning

Supervised learning is part of machine learning where the model is trained with a set of labeled data. It means that the model is learned with the previously available input and output or the data and the model tries to predict the output for the new input. If the output is closer to the expected output it works otherwise it is an error and the model tries to reduce the error.

## Unsupervised Learning

Unsupervised learning is where the model predicts the data without the help of the past dataset. It just predicts the output.

## Deep Learning

Deep learning is part of machine learning which uses deep neural networks to predict the output. It has many different layers with many number of neurons. It can be trained on both supervised and unsupervised data. This is called semi-supervised learning. It is usually trained on supervised data first and then on unsupervised data.

## Generative AI

Generative AI is a part of deep learning. It uses neural networks to generate new content and predicts the next coming data. They are trained on supervised data and unsupervised data.

## Discriminative & Generative model

Descriminative model is a part of deep learning. It uses neural networks but are mostly trained on supervised data. It is used to predict the labels for the data points. It is first trained on the data points and the labels and then it predicts the labels for the new data points. For eg. It is used to classify images.

Generative model is part of deep learning. It also uses neural networks and are trained on supervised and unsupervised data. It generates the new points based on the probability distribution. For eg. It is used to generate images.

NOTE: A machine learning model learns the relationship between the data and the label provided. On the other hand Gen AI learns the data for predicting the new content.

## LLM (Large Language Model)

LLMs are part of generative AI. They are used to generate the content which are based on human like. They predicts the output based on the highest probaility of outcome. Inside they uses transformers. Transformers has an encoder and a decoder. Hallucinations or what we call giving vague data happens when a model is trained on less data, dirty data or when contraints are not correct.

## NLP (Natural Language Processing)

NLP is a part of LLM and it creates text input to text output data which is human readable and very human like. Eg. translating languages, summarising, etc. They can be further trained for text to task outputs like replying to questions, creating docs, etc.

## Multimodal or vision diffusion models

Multimodals are used to generate text to image or text to video outputs. They usually are trained on the video or images with labels.
